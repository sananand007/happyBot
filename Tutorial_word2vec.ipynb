{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tips: https://stackoverflow.com/questions/23317458/how-to-remove-punctuation\n",
    "# copy right from TensorFlow team\n",
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "# with customization\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the file 205811889\n"
     ]
    }
   ],
   "source": [
    "url='http://mattmahoney.net/dc/'\n",
    "\n",
    "def DownloadData(filename):\n",
    "    path = os.getcwd()\n",
    "    local_filename = os.path.join(path,filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ =urllib.request.urlretrieve(url+filename,local_filename)\n",
    "    statinfo = os.stat(local_filename)\n",
    "    print('the size of the file', statinfo.st_size)\n",
    "    return local_filename\n",
    "\n",
    "filename=DownloadData('en_US.news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size 20548326\n"
     ]
    }
   ],
   "source": [
    "def readdata(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "def readdata2(filename, remove_stopwords = True):\n",
    "    with open(filename,'r+',encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        data = []\n",
    "        toker = RegexpTokenizer(r'\\w+')\n",
    "        for line in lines:\n",
    "            line = line.strip().lower()\n",
    "            data.extend(toker.tokenize(line))\n",
    "        if remove_stopwords:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            data = list(filter(lambda x: x not in stop_words, data))\n",
    "    return data\n",
    "\n",
    "volcabulary = readdata2(filename)\n",
    "print('data size', len(volcabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alone', 'apparently', 'st', 'louis', 'plant', 'close', 'would', 'die', 'old']\n",
      "Sample Data [36, 1276, 1869, 71, 256, 791, 313, 4, 2657, 58] ['home', 'alone', 'apparently', 'st', 'louis', 'plant', 'close', 'would', 'die', 'old']\n"
     ]
    }
   ],
   "source": [
    "print(volcabulary[1:10])\n",
    "volcabulary_size = 50000\n",
    "'''\n",
    "data - list of codes(integers ie, 0 - volcabulary_size-1)\n",
    "count - map of words(strings) to count the cooccurence\n",
    "refdictionary - map of words(strings) to their codes(int) -> for word2int\n",
    "reversed_refdictionary - map codes(int) to the words -> int2word\n",
    "'''\n",
    "def builddataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words-1)) #unigram count only\n",
    "    refdictionary = dict()\n",
    "    for word, _ in count:\n",
    "        refdictionary[word]=len(refdictionary)\n",
    "    data=list()\n",
    "    unk_count=0\n",
    "    for word in words:\n",
    "        index=refdictionary.get(word, 0)\n",
    "        if index==0:#refdictionary['UNK']\n",
    "            unk_count+=1\n",
    "        data.append(index)\n",
    "    count[0][1]=unk_count\n",
    "    reversed_refdictionary = dict(zip(refdictionary.values(),refdictionary.keys()))\n",
    "    return data, count, refdictionary, reversed_refdictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = builddataset(volcabulary, volcabulary_size)\n",
    "del volcabulary #clear memory\n",
    "print('Sample Data', data[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1276 alone -> 1869 apparently\n",
      "1276 alone -> 36 home\n",
      "1869 apparently -> 1276 alone\n",
      "1869 apparently -> 71 st\n",
      "71 st -> 256 louis\n",
      "71 st -> 1869 apparently\n",
      "256 louis -> 71 st\n",
      "256 louis -> 791 plant\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "batch_size = Batch size\n",
    "num_skips = How many words to consider left and right\n",
    "num_skips = How many times to reuse the input to generate the label\n",
    "num_sampled = Number of negative examples to sample\n",
    "'''\n",
    "\n",
    "data_index=0\n",
    "def gen_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size%num_skips ==0\n",
    "    assert num_skips <=2*skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
    "\n",
    "    span = 2*skip_window+1 # window seen  maximum (case of skip_window=1, span=3)\n",
    "    buffer1=collections.deque(maxlen=span) #window\n",
    "    if data_index+span>len(data):\n",
    "        data_index=0\n",
    "    buffer1.extend(data[data_index:data_index+span])\n",
    "    data_index+=span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w!= skip_window] # not taking the center word always ie, when w=1(considering skip_window=1)\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i*num_skips+j]=buffer1[skip_window] # buffer1[1] --> alwasys fixed\n",
    "            labels[i*num_skips+j,0]=buffer1[context_word] # buffer1[0] and buffer1[2] both cotext words\n",
    "        if data_index == len(data):\n",
    "            buffer1[:]=data[:span]\n",
    "            data_index=span\n",
    "        else:\n",
    "            buffer1.append(data[data_index])\n",
    "            data_index+=1\n",
    "    #Backtrack to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index+len(data)-span)%len(data)\n",
    "    return batch, labels\n",
    "\n",
    "# Example is given with batch size = 8 , we can go with bigger batch sizes as well\n",
    "batch, labels = gen_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i,0], reverse_dictionary[labels[i,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  288.51953125\n",
      "Nearest to world: atkins, bryant, leonard, holton, syringe, vermin, spurs, flamboyant,\n",
      "Nearest to 11: citation, trampoline, cheeseburger, palestinians, gaunt, disclosure, branum, callenbach,\n",
      "Nearest to percent: trolls, statuette, censorship, aquino, phylicia, hideaway, frou, 9463,\n",
      "Nearest to two: pare, greenwood, andreasen, groping, crocodile, compromise, clowning, canoeing,\n",
      "Nearest to school: housekeeping, bandmate, endocrine, donnelly, spasms, taster, basmati, minds,\n",
      "Nearest to new: lundqvist, unrelated, 1936, methods, lei, deluca, discredit, coolant,\n",
      "Nearest to 2: bravo, basing, officials, rincon, newsstands, superpower, kenjon, roc,\n",
      "Nearest to money: tahoe, cochran, 1001, 1873, bolster, overseer, pained, palestinian,\n",
      "Nearest to another: stewardship, powerless, overused, mde, pettit, mafia, lahood, gehrig,\n",
      "Nearest to year: bimbo, warehouses, upswing, lament, tour, 482, linguistic, animas,\n",
      "Nearest to part: manicured, lingers, 2100, clark, newsmagazine, bromance, perished, neuheisel,\n",
      "Nearest to way: binds, strides, sat, jew, comb, mha, modesty, woodsy,\n",
      "Nearest to three: sac, giannetti, laurence, pitney, disallowed, lunacy, concluded, brewster,\n",
      "Nearest to around: years, biotechnology, chalked, solon, inhaled, marco, fogarty, tynan,\n",
      "Nearest to us: aqap, merrill, antigovernment, noninvasive, editor, consignment, chili, drawn,\n",
      "Nearest to 7: integra, grater, 555, planks, 830, marlena, townhouse, considered,\n",
      "Average loss at step  2000 :  127.566310769\n",
      "Average loss at step  4000 :  58.5124421158\n",
      "Average loss at step  6000 :  36.059822325\n",
      "Average loss at step  8000 :  25.2576764584\n",
      "Average loss at step  10000 :  18.826450578\n",
      "Nearest to world: bryant, pho, leonard, dns, spurs, municipal, belly, participate,\n",
      "Nearest to 11: dns, disclosure, 9, retained, weston, citation, react, terrific,\n",
      "Nearest to percent: 47, universal, aquino, 8, suggested, censorship, ages, dns,\n",
      "Nearest to two: praise, compromise, imposed, outs, almost, four, question, thought,\n",
      "Nearest to school: distillery, Ã¸the, vegetable, tent, available, minds, disappointing, dns,\n",
      "Nearest to new: ritenour, UNK, unrelated, pa, thousands, financing, methods, incomes,\n",
      "Nearest to 2: 30, paddle, bravo, 1, 20, 9, municipalities, officials,\n",
      "Nearest to money: tahoe, eliminate, interesting, hitting, owned, searching, cohen, office,\n",
      "Nearest to another: stewardship, skill, vote, chorus, postcard, crane, thing, gs,\n",
      "Nearest to year: drivers, investigator, abound, oct, age, mexico, hot, redemption,\n",
      "Nearest to part: clark, helen, blamed, multi, dallas, lingers, letter, heard,\n",
      "Nearest to way: paragraph, sat, married, congressional, 235, comb, country, musicians,\n",
      "Nearest to three: opening, agencies, sac, concluded, ben, nymex, benefited, pollutants,\n",
      "Nearest to around: years, solon, anyone, eventually, raheem, bush, mitt, since,\n",
      "Nearest to us: drawn, editor, train, systems, chili, business, UNK, outfits,\n",
      "Nearest to 7: considered, bacon, 555, integra, bottle, newcomer, happens, pok,\n",
      "Average loss at step  12000 :  14.8629269276\n",
      "Average loss at step  14000 :  12.3184784713\n",
      "Average loss at step  16000 :  10.454350008\n",
      "Average loss at step  18000 :  9.0417324338\n",
      "Average loss at step  20000 :  8.42389845228\n",
      "Nearest to world: bryant, pho, leonard, spurs, holton, municipal, rings, team,\n",
      "Nearest to 11: 9, dns, react, disclosure, retained, citation, ponder, weston,\n",
      "Nearest to percent: 47, universal, 8, aquino, censorship, suggested, cardio, ages,\n",
      "Nearest to two: four, three, outs, six, seven, compromise, imposed, batters,\n",
      "Nearest to school: Ã¸the, distillery, tent, taster, vegetable, disappointing, barnum, housekeeping,\n",
      "Nearest to new: burlap, ritenour, incomes, pa, bainbridge, financing, methods, questioned,\n",
      "Nearest to 2: 1, 30, paddle, 9, 20, bravo, 3, 4,\n",
      "Nearest to money: cochran, tahoe, interesting, bloomington, hitting, eliminate, cohen, spiro,\n",
      "Nearest to another: postcard, stewardship, skill, thing, crane, tenorio, chorus, gmail,\n",
      "Nearest to year: years, oct, investigator, age, percent, gesturing, redemption, drivers,\n",
      "Nearest to part: clark, helen, lingers, 2100, blamed, tigard, secrecy, dallas,\n",
      "Nearest to way: strides, paragraph, 235, lea, longmont, binds, country, sat,\n",
      "Nearest to three: two, five, four, opening, pollutants, dissidents, stroll, sac,\n",
      "Nearest to around: years, solon, raheem, anyone, mitt, yield, acquire, salazar,\n",
      "Nearest to us: drawn, aqap, outfits, editor, business, pants, train, boynton,\n",
      "Nearest to 7: considered, 1, 8, 555, bacon, newcomer, million, integra,\n",
      "Average loss at step  22000 :  7.6654658041\n",
      "Average loss at step  24000 :  7.12294503045\n",
      "Average loss at step  26000 :  6.80195206523\n",
      "Average loss at step  28000 :  6.43202267051\n",
      "Average loss at step  30000 :  6.25729267335\n",
      "Nearest to world: bryant, pho, holton, atkins, leonard, rings, spurs, probably,\n",
      "Nearest to 11: 9, dns, 6, react, retained, disclosure, ponder, weston,\n",
      "Nearest to percent: million, 47, year, 8, aquino, universal, ages, 3,\n",
      "Nearest to two: three, four, six, five, seven, outs, one, guttenberg,\n",
      "Nearest to school: Ã¸the, distillery, tent, deviate, disappointing, msu, donnelly, students,\n",
      "Nearest to new: burlap, bainbridge, incomes, financing, ritenour, surgically, pa, zeroed,\n",
      "Nearest to 2: 1, 3, 4, 30, 9, 20, paddle, 5,\n",
      "Nearest to money: cochran, tahoe, bloomington, spiro, interesting, bolster, 1001, juniper,\n",
      "Nearest to another: postcard, stewardship, thing, skill, bluebird, crane, tenorio, mde,\n",
      "Nearest to year: years, week, percent, bitch, age, dns, month, investigator,\n",
      "Nearest to part: helen, clark, newsmagazine, lingers, tigard, secrecy, manicured, narrowly,\n",
      "Nearest to way: strides, 235, binds, paragraph, torre, lea, country, longmont,\n",
      "Nearest to three: two, four, five, hostetler, pollutants, microfracture, opening, couple,\n",
      "Nearest to around: solon, raheem, yield, acquire, years, salazar, anyone, lf,\n",
      "Nearest to us: outfits, aqap, drawn, know, business, boynton, editor, pants,\n",
      "Nearest to 7: 1, 8, considered, million, 5, 21, 555, newcomer,\n",
      "Average loss at step  32000 :  6.10028725266\n",
      "Average loss at step  34000 :  5.92189122224\n",
      "Average loss at step  36000 :  5.79130773425\n",
      "Average loss at step  38000 :  5.63163269687\n",
      "Average loss at step  40000 :  5.57321823931\n",
      "Nearest to world: bryant, atkins, rings, holton, leonard, pho, occupations, team,\n",
      "Nearest to 11: 9, dns, 6, retained, react, ponder, 3, 7,\n",
      "Nearest to percent: million, 47, year, billion, 8, 000, aquino, universal,\n",
      "Nearest to two: three, four, six, five, seven, one, outs, eight,\n",
      "Nearest to school: Ã¸the, distillery, students, deviate, tent, msu, disappointing, aye,\n",
      "Nearest to new: burlap, bainbridge, zeroed, incomes, ritenour, surgically, tidbits, financing,\n",
      "Nearest to 2: 1, 3, 4, 30, 9, 5, 20, 6,\n",
      "Nearest to money: cochran, bloomington, tahoe, 1001, spiro, interesting, bolster, able,\n",
      "Nearest to another: postcard, thing, bluebird, skill, stewardship, one, crane, tenorio,\n",
      "Nearest to year: years, week, month, percent, season, bitch, age, dns,\n",
      "Nearest to part: one, helen, newsmagazine, clark, riots, neuheisel, tigard, lingers,\n",
      "Nearest to way: strides, country, certainly, binds, 235, hennessy, longmont, torre,\n",
      "Nearest to three: two, four, five, six, several, eight, couple, hostetler,\n",
      "Nearest to around: raheem, acquire, solon, salazar, staying, fogarty, anyone, yield,\n",
      "Nearest to us: know, aqap, outfits, business, boynton, daffy, drawn, merrill,\n",
      "Nearest to 7: 1, 8, 5, 2, 21, 555, 19, 4,\n",
      "Average loss at step  42000 :  5.52296344543\n",
      "Average loss at step  44000 :  5.48464585447\n",
      "Average loss at step  46000 :  5.37899306989\n",
      "Average loss at step  48000 :  5.35151838446\n",
      "Average loss at step  50000 :  5.30367855382\n",
      "Nearest to world: bryant, rings, holton, atkins, occupations, pho, leonard, team,\n",
      "Nearest to 11: 9, dns, 6, 7, 3, 4, ponder, retained,\n",
      "Nearest to percent: million, 47, 000, billion, 8, year, universal, ages,\n",
      "Nearest to two: three, four, five, six, seven, one, eight, outs,\n",
      "Nearest to school: schools, Ã¸the, students, deviate, distillery, aye, tent, profile,\n",
      "Nearest to new: mepco, burlap, incomes, zeroed, tidbits, bainbridge, surgically, ritenour,\n",
      "Nearest to 2: 1, 3, 4, 9, 5, 6, 30, 8,\n",
      "Nearest to money: cochran, bloomington, tahoe, 1001, spiro, able, bolster, would,\n",
      "Nearest to another: one, postcard, thing, skill, bluebird, crane, tenorio, gs,\n",
      "Nearest to year: years, week, month, bitch, season, age, percent, 1892,\n",
      "Nearest to part: one, newsmagazine, helen, clark, neuheisel, riots, lingers, elise,\n",
      "Nearest to way: strides, country, certainly, things, going, chamberlin, one, dashi,\n",
      "Nearest to three: two, four, five, six, several, 10, couple, eight,\n",
      "Nearest to around: acquire, raheem, staying, salazar, turn, solon, fogarty, see,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to us: know, going, everybody, way, outfits, aqap, could, boynton,\n",
      "Nearest to 7: 1, 8, 5, 2, 6, 4, 21, 9,\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64      # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size,1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(tf.random_uniform([volcabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        #Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([volcabulary_size, embedding_size], stddev=1.0/math.sqrt(embedding_size)))\n",
    "        nce_biases=tf.Variable(tf.zeros([volcabulary_size]))\n",
    "\n",
    "    #Average loss for the batch\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(\n",
    "    weights=nce_weights,\n",
    "    biases=nce_biases,\n",
    "    labels=train_labels,\n",
    "    inputs=embed,\n",
    "    num_sampled=num_sampled,\n",
    "    num_classes=volcabulary_size\n",
    "    )\n",
    "    )\n",
    "\n",
    "    #Construct the SGD optimizer using a learning rate = 1.0\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "# %% Begin Training\n",
    "num_steps=50001\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = gen_batch(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step ', step, ': ', average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
