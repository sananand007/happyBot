{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(data, onehot_dim):\n",
    "    X = np.zeros(shape = (data.shape[0], onehot_dim))\n",
    "    for i in range(data.shape[0]):\n",
    "        X[i][data[i]] = 1 \n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 2000        # Number of possible words\n",
    "n_hidden = 300        # Number of hidden layer nodes\n",
    "with g.as_default():\n",
    "    tf_x = tf.placeholder(tf.float32, shape = (None, n_words), name = 'center_word_vec')\n",
    "    # tf_x = tf.placeholder(tf.int32, shape = (None), name = 'center_word_vec')\n",
    "    \n",
    "    # x_onehot = tf.one_hot(indices = tf_x, depth = n_words, name = 'onehot_x')\n",
    "\n",
    "    tf_y = tf.placeholder(tf.int32, shape = (None), name = 'labels')\n",
    "\n",
    "    y_onehot = tf.one_hot(indices = tf_y, depth = n_words, name = 'onehot_y')\n",
    "\n",
    "    h1 = tf.layers.dense(inputs = tf_x, units = n_hidden, activation = None, name = 'hidden_1')\n",
    "\n",
    "    h2 = tf.layers.dense(inputs = h1, units = n_words, activation = None, name = 'logits')\n",
    "\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels = y_onehot, logits = h2)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.5)\n",
    "\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = 10000        # Number of words in the training document\n",
    "data_x = np.random.randint(1,n_words, size = (num_words))\n",
    "data_y = np.random.randint(1,n_words, size = (num_words))\n",
    "X = one_hot(data_x, n_words)\n",
    "y = data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tests = np.random.randint(1, n_words, size = (100))\n",
    "# tests_input = one_hot(tests, n_words).reshape(-1, n_words)\n",
    "pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph = g) as sess:\n",
    "    sess.run(init_op)\n",
    "    training_costs = []\n",
    "    for idx in range(num_words):\n",
    "        feed = {tf_x: X[idx, :].reshape(-1, 2000), tf_y: y[idx]}\n",
    "        sess.run(train_op, feed_dict = feed)\n",
    "    # After training, evaluate the training cost for each word vector\n",
    "    for idx in range(num_words):\n",
    "        feed = {tf_x: X[idx, :].reshape(-1, 2000), tf_y: y[idx]}\n",
    "        training_costs.append(sess.run(loss, feed_dict = feed))\n",
    "    # preds = sess.run(h2, feed_dict = {tf_x: tests_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred:\n",
    "    predictions = np.argmax(preds, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the training error rate from the random data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.6636683288007976"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(training_costs)/len(training_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the loss of random guessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6009024595420822"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
