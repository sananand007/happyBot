{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import word_tokenize\n",
    "import re,string\n",
    "import time\n",
    "import numpy as np\n",
    "from getProcess import parsecorpus, dataProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a parser. Cleaning the text and create data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volcabulary size is 63579\n"
     ]
    }
   ],
   "source": [
    "path1 = 'en_US.news.txt'\n",
    "parser = parsecorpus()\n",
    "myIterator = dataProcessing(parser, corpus_path = path1, filtered_path = 'news_filtered.txt', \n",
    "                            filter_size = 0.03, n_gram = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of bi-grams we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30307"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.doc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"center_word_vec:0\", shape=(?, 63579), dtype=float32) Tensor(\"logits/BiasAdd:0\", shape=(?, 63579), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "n_words = parser.v_count        # Number of possible words\n",
    "n_hidden = 500                 # Number of hidden layer nodes\n",
    "with g.as_default():\n",
    "    tf_x = tf.placeholder(tf.float32, shape = (None, n_words), name = 'center_word_vec')\n",
    "\n",
    "    tf_y = tf.placeholder(tf.int32, shape = (None, n_words), name = 'labels')\n",
    "\n",
    "    h1 = tf.layers.dense(inputs = tf_x, units = n_hidden, activation = None, name = 'hidden_1')\n",
    "\n",
    "    h2 = tf.layers.dense(inputs = h1, units = n_words, activation = None, name = 'logits')\n",
    "    \n",
    "    print (tf_x, h2)\n",
    "\n",
    "    predictions = {'probabilities': tf.nn.softmax(h2, name = 'probabilities'), 'label': tf.argmax(h2, axis = 1, name = 'word_index')}\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels = tf_y, logits = h2), name='cross_entropy_loss')\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.5)\n",
    "\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 26000\n",
    "with tf.Session(graph = g) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(init_op)\n",
    "    for idx in range(num_words):\n",
    "        if idx > 0 and idx % 2000 == 0:\n",
    "            print ('training with %d -th word...' % idx)\n",
    "        x, y = next(myIterator)\n",
    "        feed = {tf_x: x.reshape(-1, n_words), tf_y: y.reshape(-1, n_words)}\n",
    "        cost, _ = sess.run([loss, train_op], feed_dict = feed)\n",
    "    saver.save(sess, './my_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing and calculating prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = tf.Graph()\n",
    "test_size = 5000\n",
    "y_test = []\n",
    "x_test = []\n",
    "with tf.Session(graph = g) as sess:\n",
    "    saver = tf.train.import_meta_graph('my_model.ckpt.meta')\n",
    "    saver.restore(sess, 'my_model.ckpt')\n",
    "    preds = []\n",
    "    for i in range(test_size):\n",
    "        x_t, y_t = next(myIterator)\n",
    "        y_test.append(y_t)\n",
    "        x_test.append(x_t)\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    feed = {'center_word_vec:0': x_test.reshape(-1, n_words)}\n",
    "    preds.append(sess.run('logits/BiasAdd:0', feed_dict = feed))\n",
    "    preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.reshape((-1, parser.v_count))\n",
    "predictions = np.argmax(preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.reshape((-1, parser.v_count))\n",
    "y_test = np.argmax(y_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(predictions==y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
